{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ac867d",
   "metadata": {},
   "source": [
    "## Topic Detect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12acabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bert_path = '../src/models/text2vec-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b476d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "class SentenceBert():\n",
    "    def __init__(self, embed_model):\n",
    "        super().__init__()\n",
    "        self.embedding = AutoModel.from_pretrained(embed_model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(embed_model)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        inputs = self.tokenizer(\n",
    "            sentences,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding(**inputs)\n",
    "\n",
    "        token_embed = outputs.last_hidden_state      # (Batch, T, H)\n",
    "        attention_mask = inputs['attention_mask']    # (Batch, T)\n",
    "\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embed.size()).float()\n",
    "        sentence_embed = (token_embed * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        sentence_embed = F.normalize(sentence_embed, p=2, dim=1)\n",
    "\n",
    "        return sentence_embed\n",
    "\n",
    "model = SentenceBert(embed_model=sentence_bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb6f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class SentenceEmbeddingModel():\n",
    "    def __init__(self, model_path, device=None):\n",
    "        super().__init__()\n",
    "        self.model_path = model_path\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"ä½¿ç”¨è®¾å¤‡: {self.device}\")\n",
    "        \n",
    "        # ä½¿ç”¨sentence_transformersï¼Œæ”¯æŒGPUåŠ é€Ÿ\n",
    "        self.model = SentenceTransformer(model_path, device=self.device)\n",
    "    \n",
    "    def forward(self, sentences, batch_size=64, show_progress=True):\n",
    "        \"\"\"\n",
    "        æ‰¹é‡ç”Ÿæˆembeddingsï¼Œæ”¯æŒè¿›åº¦æ˜¾ç¤º\n",
    "        \n",
    "        Args:\n",
    "            sentences: æ–‡æœ¬åˆ—è¡¨\n",
    "            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ŒGPUå¯ç”¨æ—¶å¯ä»¥è®¾ä¸º64-128\n",
    "            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(\n",
    "            sentences,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=show_progress,\n",
    "            normalize_embeddings=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156b0439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding outputs:\n",
      " tensor([[-0.0115, -0.0204,  0.0322,  ...,  0.0142,  0.0103, -0.0010],\n",
      "        [ 0.0088, -0.0345,  0.0342,  ...,  0.0062,  0.0138,  0.0177]])\n",
      "\n",
      " torch.Size([2, 768])\n",
      "tensor(0.8632)\n"
     ]
    }
   ],
   "source": [
    "# Demo to test ShortTopicDetect class\n",
    "\n",
    "sentences = ['ä»Šå¤©çš„å¤©æ°”çœŸå¥½', 'ä»Šå¤©å¤©æ°”ä¸é”™']\n",
    "outputs = model.forward(sentences)\n",
    "print('Embedding outputs:\\n', outputs)\n",
    "print('\\n', outputs.shape)\n",
    "print(outputs[0] @ outputs[1])  # Cosine similarity between two sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1dde54",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "### HDBSCAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03aea80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "class HDBSCAN():\n",
    "    def __init__(self, min_cluster_size=100, min_samples=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            min_cluster_size: æœ€å°èšç±»å¤§å°ï¼Œå»ºè®®50-200ï¼Œæ•°æ®é‡å¤§æ—¶è°ƒå¤§\n",
    "            min_samples: æœ€å°æ ·æœ¬æ•°ï¼Œé»˜è®¤ä¸ºmin_cluster_sizeçš„ä¸€åŠ\n",
    "        \"\"\"\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.min_samples = min_samples if min_samples else min_cluster_size // 2\n",
    "\n",
    "        self.clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=self.min_cluster_size,\n",
    "            min_samples=self.min_samples,\n",
    "            metric='euclidean',  # å½’ä¸€åŒ–åçš„å‘é‡ï¼Œeuclideanç­‰ä»·äºcosine\n",
    "            prediction_data=True,\n",
    "            cluster_selection_method='eom'\n",
    "        )\n",
    "        self.labels_ = None\n",
    "        self.topic_centers_ = {}\n",
    "        self.topic_sizes_ = {}\n",
    "    \n",
    "    def fit(self, embeddings):\n",
    "        # å½’ä¸€åŒ–embeddingsï¼Œä½¿å…¶åœ¨å•ä½çƒé¢ä¸Š\n",
    "        embeddings = normalize(embeddings)\n",
    "\n",
    "        self.clusterer.fit(embeddings)\n",
    "        self.labels_ = self.clusterer.labels_\n",
    "        self._compute_topic_centers(embeddings)\n",
    "\n",
    "        return self.labels_\n",
    "\n",
    "    def _compute_topic_centers(self, embeddings):\n",
    "        topic_vectors = defaultdict(list)\n",
    "\n",
    "        for vec, label in zip(embeddings, self.labels_):\n",
    "            if label == -1:\n",
    "                continue  # å¿½ç•¥å™ªå£°\n",
    "            topic_vectors[label].append(vec)\n",
    "\n",
    "        self.topic_centers_ = {}\n",
    "        self.topic_sizes_ = {}\n",
    "\n",
    "        for label, vecs in topic_vectors.items():\n",
    "            vecs = np.vstack(vecs)\n",
    "            center = vecs.mean(axis=0)\n",
    "            center = center / np.linalg.norm(center)\n",
    "\n",
    "            self.topic_centers_[label] = center\n",
    "            self.topic_sizes_[label] = len(vecs)\n",
    "    \n",
    "    def get_topics(self):\n",
    "        topics = []\n",
    "        for label in self.topic_centers_:\n",
    "            topics.append({\n",
    "                \"topic_id\": label,\n",
    "                \"size\": self.topic_sizes_[label],\n",
    "                \"center\": self.topic_centers_[label]\n",
    "            })\n",
    "        # æŒ‰sizeæ’åº\n",
    "        topics.sort(key=lambda x: x['size'], reverse=True)\n",
    "        return topics\n",
    "\n",
    "    def predict(self, new_embeddings):\n",
    "        new_embeddings = normalize(new_embeddings)\n",
    "\n",
    "        labels, _ = hdbscan.approximate_predict(\n",
    "            self.clusterer,\n",
    "            new_embeddings\n",
    "        )\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2b316",
   "metadata": {},
   "source": [
    "### Experiment on Weibo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "413caa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ€»æ•°æ®é‡: 30341 æ¡\n",
      "\n",
      "ç¤ºä¾‹æ•°æ®:\n",
      "{\n",
      "  \"source\": \"weibo\",\n",
      "  \"topic\": \"%E7%BA%A2%E5%8C%85\",\n",
      "  \"weibo_id\": \"5266154761489193\",\n",
      "  \"text\": \"#æƒ…äººèŠ‚å¾®ä¿¡å¼€æ”¾520å…ƒçº¢åŒ…# éƒ½å»è¯•ä¸€ä¸‹ï¼Œä»Šå¤©çœŸçš„å¯ä»¥è½¬520çš„å¤§çº¢åŒ…è¯¶ \",\n",
      "  \"url\": \"https://weibo.com/1810074673/5266154761489193\",\n",
      "  \"timestamp\": \"Sat Feb 14 09:29:58 +0800 2026\",\n",
      "  \"reposts\": 8,\n",
      "  \"comments\": 49,\n",
      "  \"likes\": 177\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '../html/data/weibo_data.json'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"æ€»æ•°æ®é‡: {len(data)} æ¡\")\n",
    "print(\"\\nç¤ºä¾‹æ•°æ®:\")\n",
    "print(json.dumps(data[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0329664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ‰æ•ˆæ–‡æœ¬æ•°: 30341 æ¡\n",
      "\n",
      "æ•°æ®æ¥æº (æŒ‰æœç´¢å…³é”®è¯):\n",
      "  å¾®ä¿¡å›åº”çº¢åŒ…æ‰‹æ°”æœ€ä½³æ”»ç•¥: 1250 æ¡\n",
      "  çœ‹ç”µå½±çš„å¿«ä¹æ­£å…¨é¢å‡çº§: 1225 æ¡\n",
      "  çº¢åŒ…: 1224 æ¡\n",
      "  æƒ…äººèŠ‚: 1200 æ¡\n",
      "  2026NBAå…¨æ˜æ˜Ÿ: 1125 æ¡\n",
      "  ç”Ÿå‘½æ ‘è±†ç“£å¼€åˆ†8.1: 1100 æ¡\n",
      "  æª€å¥æ¬¡å¢æ˜±æ™“å‰§å®£: 1081 æ¡\n",
      "  çªç„¶å‘ç°æœ‹å‹åœˆæ²¡äººæ™’èŠ±äº†: 1058 æ¡\n",
      "  é»„å­éŸ¬å€Ÿç»¼è‰ºæ‰‡äº†å¾è‰ºæ´‹: 1000 æ¡\n",
      "  æš–é˜³å‘æ–‡: 975 æ¡\n",
      "ä½¿ç”¨è®¾å¤‡: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [11:33<00:00,  5.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings shape: (30341, 768)\n",
      "\n",
      "========== æ–¹æ¡ˆ1: å…¨å±€èšç±» ==========\n",
      "min_cluster_size=100: 60 clusters, 6413 noise (21.1%)\n",
      "min_cluster_size=200: 34 clusters, 8888 noise (29.3%)\n",
      "min_cluster_size=300: 25 clusters, 10986 noise (36.2%)\n",
      "\n",
      "========== èšç±»ç»“æœ ==========\n",
      "æ€»æ•°æ®é‡: 30341\n",
      "å‘ç°topicæ•°: 34\n",
      "å™ªå£°æ•°æ®: 8888 æ¡ (29.3%)\n",
      "\n",
      "========== Top 10 Topics ==========\n",
      "Topic 31: 1490 æ¡\n",
      "Topic 9: 1100 æ¡\n",
      "Topic 17: 1035 æ¡\n",
      "Topic 23: 975 æ¡\n",
      "Topic 25: 951 æ¡\n",
      "Topic 7: 924 æ¡\n",
      "Topic 1: 900 æ¡\n",
      "Topic 0: 860 æ¡\n",
      "Topic 13: 851 æ¡\n",
      "Topic 24: 840 æ¡\n",
      "\n",
      "========== Top 5 Topics ä»£è¡¨æ€§æ–‡æœ¬ ==========\n",
      "\n",
      "--- Topic 31 (å…±1490æ¡) ---\n",
      "  [855] #ç½‘å‹å¶é‡äº†æ˜“çƒŠåƒçº#å¥½å¸…å•Šå•Šå•Šå•Šå¤§å¹´åˆä¸€çœ‹ã€ŠæƒŠè›°æ— å£°ã€‹ï¼ï¼ï¼ ...\n",
      "  [199] #ç½‘å‹å¶é‡äº†æ˜“çƒŠåƒçº#å¥½å¸…å•Šå•Šå•Šå•Šå¤§å¹´åˆä¸€çœ‹ã€ŠæƒŠè›°æ— å£°ã€‹ï¼ï¼ï¼ ...\n",
      "  [1429] #ç½‘å‹å¶é‡äº†æ˜“çƒŠåƒçº#å¥½å¸…å•Šå•Šå•Šå•Šå¤§å¹´åˆä¸€çœ‹ã€ŠæƒŠè›°æ— å£°ã€‹ï¼ï¼ï¼ ...\n",
      "\n",
      "--- Topic 9 (å…±1100æ¡) ---\n",
      "  [1071] #ç”Ÿå‘½æ ‘è±†ç“£å¼€åˆ†8.1#è¿™ä¹ˆå¥½çš„å‰§å€¼å¾—9åˆ†ä»¥ä¸Šï¼Œé‚£è¡Œç²‰åœˆçš„é»‘å­çˆ¬è¿œä¸€ç‚¹ï¼ç”Ÿå‘½æ ‘æ¨ç´« ...\n",
      "  [46] #ç”Ÿå‘½æ ‘è±†ç“£å¼€åˆ†8.1#è¿™ä¹ˆå¥½çš„å‰§å€¼å¾—9åˆ†ä»¥ä¸Šï¼Œé‚£è¡Œç²‰åœˆçš„é»‘å­çˆ¬è¿œä¸€ç‚¹ï¼ç”Ÿå‘½æ ‘æ¨ç´« ...\n",
      "  [96] #ç”Ÿå‘½æ ‘è±†ç“£å¼€åˆ†8.1#è¿™ä¹ˆå¥½çš„å‰§å€¼å¾—9åˆ†ä»¥ä¸Šï¼Œé‚£è¡Œç²‰åœˆçš„é»‘å­çˆ¬è¿œä¸€ç‚¹ï¼ç”Ÿå‘½æ ‘æ¨ç´« ...\n",
      "\n",
      "--- Topic 17 (å…±1035æ¡) ---\n",
      "  [8] NBAç‹é¹¤æ££-æˆ‘çš„è¯„åˆ†ï¼š#2026NBAå…¨æ˜æ˜Ÿ##NBAå…¨æ˜æ˜Ÿåäººèµ›# ...\n",
      "  [113] NBAç‹é¹¤æ££-æˆ‘çš„è¯„åˆ†ï¼š#2026NBAå…¨æ˜æ˜Ÿ##NBAå…¨æ˜æ˜Ÿåäººèµ›# ...\n",
      "  [638] NBAç‹é¹¤æ££-æˆ‘çš„è¯„åˆ†ï¼š#2026NBAå…¨æ˜æ˜Ÿ##NBAå…¨æ˜æ˜Ÿåäººèµ›# ...\n",
      "\n",
      "--- Topic 23 (å…±975æ¡) ---\n",
      "  [923] #çªç„¶å‘ç°æœ‹å‹åœˆæ²¡äººæ™’èŠ±äº†#æƒ…äººèŠ‚æœ‹å‹åœˆæ™’ç¤¼ç‰©ï¼Œæ™’çº¢åŒ…çš„é‚£äº›äººéƒ½å¼±çˆ†äº† æœ‰æœ¬äº‹æŠŠä½ ä»¬çš„æƒ…äººæ™’å‡ºæ¥çœ‹çœ‹ ...\n",
      "  [535] #çªç„¶å‘ç°æœ‹å‹åœˆæ²¡äººæ™’èŠ±äº†#æƒ…äººèŠ‚æœ‹å‹åœˆæ™’ç¤¼ç‰©ï¼Œæ™’çº¢åŒ…çš„é‚£äº›äººéƒ½å¼±çˆ†äº† æœ‰æœ¬äº‹æŠŠä½ ä»¬çš„æƒ…äººæ™’å‡ºæ¥çœ‹çœ‹ ...\n",
      "  [555] #çªç„¶å‘ç°æœ‹å‹åœˆæ²¡äººæ™’èŠ±äº†#æƒ…äººèŠ‚æœ‹å‹åœˆæ™’ç¤¼ç‰©ï¼Œæ™’çº¢åŒ…çš„é‚£äº›äººéƒ½å¼±çˆ†äº† æœ‰æœ¬äº‹æŠŠä½ ä»¬çš„æƒ…äººæ™’å‡ºæ¥çœ‹çœ‹ ...\n",
      "\n",
      "--- Topic 25 (å…±951æ¡) ---\n",
      "  [936] #å®‹è½¶æˆ’æŒ‡#ä¿©äººä¸æ˜¯æ²¡åœ¨ä¸€èµ·è¿‡å—ï¼Ÿå®‹è½¶åˆæ˜¯æ¢å¾®åšèƒŒæ™¯ï¼Œè¿˜æŠŠå…³æ³¨ç§å¯†äº†ï¼Œç°åœ¨åˆæŒ–å‡ºæƒ…ä¾£æˆ’æŒ‡ï¼Œå¾ˆéš¾è®©äººä¸å¤šæƒ³å•Šã€‚ å®‹è½¶æ— åæŒ‡æˆ´äº†æˆ’æŒ‡ ...\n",
      "  [516] #å®‹è½¶æˆ’æŒ‡#ä¿©äººä¸æ˜¯æ²¡åœ¨ä¸€èµ·è¿‡å—ï¼Ÿå®‹è½¶åˆæ˜¯æ¢å¾®åšèƒŒæ™¯ï¼Œè¿˜æŠŠå…³æ³¨ç§å¯†äº†ï¼Œç°åœ¨åˆæŒ–å‡ºæƒ…ä¾£æˆ’æŒ‡ï¼Œå¾ˆéš¾è®©äººä¸å¤šæƒ³å•Šã€‚ å®‹è½¶æ— åæŒ‡æˆ´äº†æˆ’æŒ‡ ...\n",
      "  [235] #å®‹è½¶æˆ’æŒ‡#ä¿©äººä¸æ˜¯æ²¡åœ¨ä¸€èµ·è¿‡å—ï¼Ÿå®‹è½¶åˆæ˜¯æ¢å¾®åšèƒŒæ™¯ï¼Œè¿˜æŠŠå…³æ³¨ç§å¯†äº†ï¼Œç°åœ¨åˆæŒ–å‡ºæƒ…ä¾£æˆ’æŒ‡ï¼Œå¾ˆéš¾è®©äººä¸å¤šæƒ³å•Šã€‚ å®‹è½¶æ— åæŒ‡æˆ´äº†æˆ’æŒ‡ ...\n",
      "\n",
      "\n",
      "========== æ–¹æ¡ˆ2: æŒ‰çƒ­æœå…³é”®è¯åˆ†ç»„ç»†åˆ† ==========\n",
      "\n",
      "--- çƒ­æœ: å¾®ä¿¡å›åº”çº¢åŒ…æ‰‹æ°”æœ€ä½³æ”»ç•¥ (å…±1250æ¡) ---\n",
      "  ç»†åˆ†å‡º 2 ä¸ªå­è¯é¢˜, å™ªå£° 950 æ¡\n",
      "    å­è¯é¢˜1 (150æ¡): #å¾®ä¿¡å›åº”çº¢åŒ…æ‰‹æ°”æœ€ä½³æ”»ç•¥#åˆšæ‰¾äº†å¾®ä¿¡æ”¯ä»˜äº§å“ç»ç†åŸæ’­å®¢çš„å…·ä½“ç‰‡æ®µï¼Œæˆªå‡ºæ¥åŠ äº†å­—å¹•ï¼Œç»™å¤§å®¶å¬ä¸‹ï¼šå¾®ä¿¡æ‰‹æœºçº¢åŒ…éƒ½æ˜¯éšæœºé‡‘é¢...\n",
      "    å­è¯é¢˜2 (150æ¡): #å¾®ä¿¡å›åº”çº¢åŒ…æ‰‹æ°”æœ€ä½³æ”»ç•¥#æŠ¢ğŸ§§çœŸçš„æ˜¯çº¯çœ‹è¿æ°” ...\n",
      "\n",
      "--- çƒ­æœ: çœ‹ç”µå½±çš„å¿«ä¹æ­£å…¨é¢å‡çº§ (å…±1225æ¡) ---\n",
      "  ç»†åˆ†å‡º 2 ä¸ªå­è¯é¢˜, å™ªå£° 588 æ¡\n",
      "    å­è¯é¢˜1 (441æ¡): #çœ‹ç”µå½±çš„å¿«ä¹æ­£å…¨é¢å‡çº§#ç°åœ¨çš„ç”µå½±ä½“éªŒï¼Œå°±æ˜¯ä¸€åœºâ€œèŠ±å°é’±çœ‹å¤§ç‰‡ + æ²‰æµ¸å¼äº«å— + ç¤¾äº¤æ‰“å¡â€çš„ç»„åˆæ‹³ã€‚ä½ æ˜¯æ‰“ç®—å»çœ‹æ˜¥...\n",
      "    å­è¯é¢˜2 (196æ¡): #çœ‹ç”µå½±çš„å¿«ä¹æ­£å…¨é¢å‡çº§#ä»Šå¹´æ˜¥èŠ‚æ¡£å…«éƒ¨ç”µå½±é›†ç»“äº†ï¼šå–œå‰§ã€è°æˆ˜ã€æ­¦ä¾ ã€ç§‘å¹»ã€åˆå®¶æ¬¢ã€åŠ¨ç”»ã€çºªå½•ã€ç²¤è¯­å–œå‰§å…¨è¦†ç›–ï¼ä½ æœ€æœŸå¾…...\n",
      "\n",
      "--- çƒ­æœ: çº¢åŒ… (å…±1224æ¡) ---\n",
      "  ç»†åˆ†å‡º 2 ä¸ªå­è¯é¢˜, å™ªå£° 663 æ¡\n",
      "    å­è¯é¢˜1 (408æ¡): æ˜¥èŠ‚&amp;æƒ…äººèŠ‚åŒèŠ‚å¿«ä¹ï¼ä¸ºå¤§ä¼™å‡†å¤‡äº†å¤§çº¢åŒ…ï¼Œæ¬¢è¿é¢†å–ï¼é™æœŸä½¿ç”¨å“¦ï¼Œè¿‡æœŸä¸è¡¥å¦ï¼Œå¯ä»¥é¡ºä¾¿æµ‹ä¸€ä¸‹è‡ªå·±çš„å¿«ä¹çŸ¥è¯†æŒæ¡æ°´å¹³...\n",
      "    å­è¯é¢˜2 (153æ¡): #å¾®ä¿¡å›åº”çº¢åŒ…æ‰‹æ°”æœ€ä½³æ”»ç•¥#åˆšæ‰¾äº†å¾®ä¿¡æ”¯ä»˜äº§å“ç»ç†åŸæ’­å®¢çš„å…·ä½“ç‰‡æ®µï¼Œæˆªå‡ºæ¥åŠ äº†å­—å¹•ï¼Œç»™å¤§å®¶å¬ä¸‹ï¼šå¾®ä¿¡æ‰‹æœºçº¢åŒ…éƒ½æ˜¯éšæœºé‡‘é¢...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# æå–æ–‡æœ¬å†…å®¹\n",
    "texts = [item['text'] for item in data if item.get('text')]\n",
    "print(f\"æœ‰æ•ˆæ–‡æœ¬æ•°: {len(texts)} æ¡\")\n",
    "\n",
    "# å…ˆçœ‹çœ‹æ•°æ®çš„æ¥æºåˆ†å¸ƒ\n",
    "topic_distribution = {}\n",
    "for item in data:\n",
    "    topic = item.get('topic', 'unknown')\n",
    "    topic_distribution[topic] = topic_distribution.get(topic, 0) + 1\n",
    "\n",
    "print(f\"\\næ•°æ®æ¥æº (æŒ‰æœç´¢å…³é”®è¯):\")\n",
    "for topic, count in sorted(topic_distribution.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    from urllib.parse import unquote\n",
    "    print(f\"  {unquote(topic)}: {count} æ¡\")\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹å¹¶ç”Ÿæˆembeddings\n",
    "model = SentenceEmbeddingModel(model_path=sentence_bert_path)\n",
    "\n",
    "# GPUå¯ç”¨æ—¶ç”¨æ›´å¤§çš„batch_sizeåŠ é€Ÿ\n",
    "batch_size = 256\n",
    "embeddings = model.forward(texts, batch_size=batch_size, show_progress=True)\n",
    "\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# ===== æ–¹æ¡ˆ1: å¯¹æ‰€æœ‰æ•°æ®èšç±» =====\n",
    "print(\"\\n========== æ–¹æ¡ˆ1: å…¨å±€èšç±» ==========\")\n",
    "# è®¾ç½®æ›´åˆç†çš„ min_cluster_size\n",
    "for min_cluster_size in [100, 200, 300]:\n",
    "    detector = HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "    labels = detector.fit(embeddings)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    print(f\"min_cluster_size={min_cluster_size}: {n_clusters} clusters, {n_noise} noise ({n_noise/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# ç”¨ min_cluster_size=200 åšæœ€ç»ˆåˆ†æ\n",
    "detector = HDBSCAN(min_cluster_size=200)\n",
    "labels = detector.fit(embeddings)\n",
    "topics = detector.get_topics()\n",
    "\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "\n",
    "print(f\"\\n========== èšç±»ç»“æœ ==========\")\n",
    "print(f\"æ€»æ•°æ®é‡: {len(labels)}\")\n",
    "print(f\"å‘ç°topicæ•°: {n_clusters}\")\n",
    "print(f\"å™ªå£°æ•°æ®: {n_noise} æ¡ ({n_noise/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# æ˜¾ç¤ºtop topics\n",
    "if n_clusters > 0:\n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"\\n========== Top 10 Topics ==========\")\n",
    "    sorted_topics = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i, (label, count) in enumerate(sorted_topics[:11]):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        print(f\"Topic {label}: {count} æ¡\")\n",
    "\n",
    "    # ä¸ºæ¯ä¸ªtop topicæ‰¾ä»£è¡¨æ€§æ–‡æœ¬\n",
    "    print(f\"\\n========== Top 5 Topics ä»£è¡¨æ€§æ–‡æœ¬ ==========\")\n",
    "    for i, topic_info in enumerate(topics[:5]):\n",
    "        topic_id = topic_info['topic_id']\n",
    "        center = topic_info['center']\n",
    "        \n",
    "        # æ‰¾åˆ°å±äºè¯¥topicçš„æ‰€æœ‰æ–‡æœ¬ç´¢å¼•\n",
    "        topic_indices = [i for i, label in enumerate(labels) if label == topic_id]\n",
    "        \n",
    "        if topic_indices:\n",
    "            # è®¡ç®—ä¸ä¸­å¿ƒçš„ç›¸ä¼¼åº¦\n",
    "            topic_embeddings = embeddings[topic_indices]\n",
    "            similarities = topic_embeddings @ center\n",
    "            \n",
    "            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„3æ¡æ–‡æœ¬\n",
    "            top_3_idx = np.argsort(similarities)[-3:][::-1]\n",
    "            print(f\"\\n--- Topic {topic_id} (å…±{len(topic_indices)}æ¡) ---\")\n",
    "            for idx in top_3_idx:\n",
    "                original_idx = topic_indices[idx]\n",
    "                print(f\"  [{idx+1}] {texts[original_idx][:80]}...\")\n",
    "\n",
    "# ===== æ–¹æ¡ˆ2: æŒ‰çƒ­æœå…³é”®è¯åˆ†ç»„åˆ†æ =====\n",
    "print(f\"\\n\\n========== æ–¹æ¡ˆ2: æŒ‰çƒ­æœå…³é”®è¯åˆ†ç»„ç»†åˆ† ==========\")\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# é€‰æ‹©æ•°æ®é‡æœ€å¤šçš„3ä¸ªå…³é”®è¯è¿›è¡Œç»†åˆ†åˆ†æ\n",
    "top_keywords = sorted(topic_distribution.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "for keyword, count in top_keywords:\n",
    "    print(f\"\\n--- çƒ­æœ: {unquote(keyword)} (å…±{count}æ¡) ---\")\n",
    "    \n",
    "    # æå–è¯¥å…³é”®è¯ä¸‹çš„æ‰€æœ‰æ–‡æœ¬\n",
    "    keyword_indices = [i for i, item in enumerate(data) if item.get('topic') == keyword]\n",
    "    keyword_texts = [texts[i] for i in keyword_indices]\n",
    "    keyword_embeddings = embeddings[keyword_indices]\n",
    "    \n",
    "    # å¯¹è¯¥å…³é”®è¯ä¸‹çš„å†…å®¹è¿›è¡Œèšç±»\n",
    "    # min_cluster_size è®¾ä¸ºè¯¥å…³é”®è¯æ•°æ®é‡çš„ 10-20%\n",
    "    mcs = max(5, int(len(keyword_texts) * 0.1))\n",
    "    detector_kw = HDBSCAN(min_cluster_size=mcs)\n",
    "    labels_kw = detector_kw.fit(keyword_embeddings)\n",
    "    \n",
    "    n_clusters_kw = len(set(labels_kw)) - (1 if -1 in labels_kw else 0)\n",
    "    n_noise_kw = list(labels_kw).count(-1)\n",
    "    \n",
    "    print(f\"  ç»†åˆ†å‡º {n_clusters_kw} ä¸ªå­è¯é¢˜, å™ªå£° {n_noise_kw} æ¡\")\n",
    "    \n",
    "    # æ˜¾ç¤ºè¯¥å…³é”®è¯ä¸‹çš„å­è¯é¢˜\n",
    "    if n_clusters_kw > 0:\n",
    "        topics_kw = detector_kw.get_topics()\n",
    "        for j, topic_info in enumerate(topics_kw[:3]):\n",
    "            topic_id = topic_info['topic_id']\n",
    "            center = topic_info['center']\n",
    "            topic_indices_kw = [k for k, label in enumerate(labels_kw) if label == topic_id]\n",
    "            \n",
    "            # æ‰¾ä»£è¡¨æ€§æ–‡æœ¬\n",
    "            topic_embs = keyword_embeddings[topic_indices_kw]\n",
    "            sims = topic_embs @ center\n",
    "            best_idx = topic_indices_kw[sims.argmax()]\n",
    "            \n",
    "            print(f\"    å­è¯é¢˜{j+1} ({len(topic_indices_kw)}æ¡): {keyword_texts[best_idx][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75c042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f4e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-poa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
