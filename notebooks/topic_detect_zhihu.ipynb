{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Detect Model in Zhihu\n",
    "``(Sentence-BERT + UMAP + HDBSCAN)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®æ˜¾ç¤ºé€‰é¡¹\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import umap\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class SentenceEmbeddingModel:\n",
    "    def __init__(self, model_path, device=None):\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = SentenceTransformer(model_path, device=self.device)\n",
    "    \n",
    "    def forward(self, sentences, batch_size=64, show_progress=True):\n",
    "        return self.model.encode(\n",
    "            sentences,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=show_progress,\n",
    "            normalize_embeddings=True,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "class UMAPReducer:\n",
    "    def __init__(self, n_components=50, n_neighbors=15, min_dist=0.1, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_dist = min_dist\n",
    "        self.random_state = random_state\n",
    "        self.reducer = None\n",
    "        \n",
    "    def fit_transform(self, embeddings):\n",
    "        self.reducer = umap.UMAP(\n",
    "            n_components=self.n_components,\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            min_dist=self.min_dist,\n",
    "            metric='cosine',\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        return self.reducer.fit_transform(embeddings)\n",
    "    \n",
    "    def transform(self, embeddings):\n",
    "        return self.reducer.transform(embeddings)\n",
    "\n",
    "\n",
    "class HDBSCANCluster:\n",
    "    def __init__(self, min_cluster_size=30, min_samples=None):\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.min_samples = min_samples or min_cluster_size // 2\n",
    "        self.clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=self.min_cluster_size,\n",
    "            min_samples=self.min_samples,\n",
    "            metric='euclidean',\n",
    "            prediction_data=True,\n",
    "            cluster_selection_method='eom'\n",
    "        )\n",
    "        self.labels_ = None\n",
    "        self.topic_centers_ = {}\n",
    "        self.topic_sizes_ = {}\n",
    "    \n",
    "    def fit(self, embeddings):\n",
    "        embeddings = normalize(embeddings)\n",
    "        self.clusterer.fit(embeddings)\n",
    "        self.labels_ = self.clusterer.labels_\n",
    "        self._compute_topic_centers(embeddings)\n",
    "        return self.labels_\n",
    "    \n",
    "    def _compute_topic_centers(self, embeddings):\n",
    "        topic_vectors = defaultdict(list)\n",
    "        for vec, label in zip(embeddings, self.labels_):\n",
    "            if label != -1:\n",
    "                topic_vectors[label].append(vec)\n",
    "        \n",
    "        for label, vecs in topic_vectors.items():\n",
    "            vecs = np.vstack(vecs)\n",
    "            center = vecs.mean(axis=0)\n",
    "            self.topic_centers_[label] = center / np.linalg.norm(center)\n",
    "            self.topic_sizes_[label] = len(vecs)\n",
    "    \n",
    "    def get_topics(self):\n",
    "        topics = [\n",
    "            {\"topic_id\": l, \"size\": self.topic_sizes_[l], \"center\": self.topic_centers_[l]}\n",
    "            for l in self.topic_centers_\n",
    "        ]\n",
    "        return sorted(topics, key=lambda x: x['size'], reverse=True)\n",
    "    \n",
    "    def predict(self, new_embeddings):\n",
    "        new_embeddings = normalize(new_embeddings)\n",
    "        labels, _ = hdbscan.approximate_predict(self.clusterer, new_embeddings)\n",
    "        return labels\n",
    "\n",
    "\n",
    "class TopicDetector:\n",
    "    def __init__(self, model_path=None, device=None, use_umap=True, \n",
    "                 umap_components=50, umap_neighbors=25, min_cluster_size=30, min_samples=None):\n",
    "        self.model_path = model_path or os.getenv('SEN_EMB_MODEL_PATH', '../src/models/text2vec-base-chinese')\n",
    "        self.embedding_model = SentenceEmbeddingModel(self.model_path, device)\n",
    "        \n",
    "        if use_umap:\n",
    "            self.umap_reducer = UMAPReducer(n_components=umap_components, n_neighbors=umap_neighbors)\n",
    "        else:\n",
    "            self.umap_reducer = None\n",
    "            \n",
    "        self.cluster_model = HDBSCANCluster(min_cluster_size, min_samples)\n",
    "        self.embeddings = None\n",
    "        self.embeddings_reduced = None\n",
    "        self.labels = None\n",
    "    \n",
    "    def fit(self, texts, batch_size=128, show_progress=True):\n",
    "        print(\"Step 1: ç”Ÿæˆå¥å­åµŒå…¥...\")\n",
    "        self.embeddings = self.embedding_model.forward(texts, batch_size, show_progress)\n",
    "        print(f\"  ç»´åº¦: {self.embeddings.shape}\")\n",
    "        \n",
    "        if self.umap_reducer:\n",
    "            print(\"Step 2: UMAP é™ç»´...\")\n",
    "            self.embeddings_reduced = self.umap_reducer.fit_transform(self.embeddings)\n",
    "        else:\n",
    "            self.embeddings_reduced = self.embeddings\n",
    "        \n",
    "        print(\"Step 3: HDBSCAN èšç±»...\")\n",
    "        self.labels = self.cluster_model.fit(self.embeddings_reduced)\n",
    "        \n",
    "        n_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)\n",
    "        n_noise = list(self.labels).count(-1)\n",
    "        print(f\"  ä¸»é¢˜: {n_clusters}, å™ªå£°: {n_noise} ({n_noise/len(self.labels)*100:.1f}%)\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, texts, **kwargs):\n",
    "        self.fit(texts, **kwargs)\n",
    "        return self.labels, self.cluster_model.get_topics()\n",
    "    \n",
    "    def get_topic_summary(self):\n",
    "        if self.labels is None:\n",
    "            return {}\n",
    "        n_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)\n",
    "        n_noise = list(self.labels).count(-1)\n",
    "        return {\n",
    "            \"total_samples\": len(self.labels),\n",
    "            \"n_topics\": n_clusters,\n",
    "            \"n_noise\": n_noise,\n",
    "            \"noise_ratio\": f\"{n_noise / len(self.labels) * 100:.1f}%\",\n",
    "        }\n",
    "    \n",
    "    def get_representative_texts(self, texts, topic_id, top_k=3):\n",
    "        center = self.cluster_model.topic_centers_[topic_id]\n",
    "        topic_indices = [i for i, l in enumerate(self.labels) if l == topic_id]\n",
    "        \n",
    "        embeddings_norm = normalize(self.embeddings_reduced)\n",
    "        similarities = embeddings_norm[topic_indices] @ center\n",
    "        top_idx = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        return [(topic_indices[i], texts[topic_indices[i]], similarities[i]) for i in top_idx]\n",
    "    \n",
    "    def extract_keywords_tfidf(self, texts, topic_id, top_k=5):\n",
    "        topic_indices = [i for i, l in enumerate(self.labels) if l == topic_id]\n",
    "        topic_texts = [texts[i] for i in topic_indices]\n",
    "        \n",
    "        if len(topic_texts) < 2:\n",
    "            return []\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(max_features=1000, token_pattern=r'(?u)\\b\\w+\\b')\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(topic_texts)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            mean_scores = tfidf_matrix.mean(axis=0).A1\n",
    "            top_indices = mean_scores.argsort()[-top_k:][::-1]\n",
    "            return [feature_names[i] for i in top_indices]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "\n",
    "def clean_text(text, max_len=150):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'#[^#\\s]+#?', '', text)\n",
    "    return text[:max_len] + '...' if len(text) > max_len else text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ€»æ•°æ®é‡: 3009 æ¡\n",
      "åœˆå­åˆ†å¸ƒ:\n",
      "ring_name\n",
      "AIä¸äººç±»æœªæ¥          466\n",
      "AI åˆ›æŠ•ç”Ÿæ€åœˆ         263\n",
      "ç§‘ç ” AI Hub        255\n",
      "ç®—æ³•ç ”ç©¶æ‰€            253\n",
      "AIå†™ä½œç ”ç©¶æ‰€          246\n",
      "AI Coding æ¢ç´¢èˆ°    241\n",
      "AI å·¥å…·æµ‹è¯„ä¸­å¿ƒ        237\n",
      "AI æ—¶ä»£çš„æˆ‘ä»¬         226\n",
      "AI å®‰å…¨ç ”ç©¶æ‰€         214\n",
      "DeepSeek æ·±æ½œèˆ±     168\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"../data\")\n",
    "data_file = data_dir / \"zhihu_ring_data_20260225_senti.json\"\n",
    "circles_file = data_dir / \"zhihu_ai_circles.json\"\n",
    "\n",
    "with open(data_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(circles_file, 'r', encoding='utf-8') as f:\n",
    "    circles_data = json.load(f)\n",
    "\n",
    "ring_to_name = {c['ring_id']: c['name'] for c in circles_data}\n",
    "df = pd.DataFrame(data)\n",
    "df['ring_name'] = df['ring_id'].map(ring_to_name).fillna(df['ring_id'])\n",
    "\n",
    "print(f\"æ€»æ•°æ®é‡: {len(data)} æ¡\")\n",
    "print(f\"åœˆå­åˆ†å¸ƒ:\\n{df['ring_name'].value_counts().head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ€»æ•°æ®é‡: 3009 æ¡\n",
      "æ¨¡å‹è·¯å¾„: d:\\Projects\\nlp-public-opinion-analysis\\src\\models\\text2vec-base-chinese\n",
      "åœˆå­åˆ†å¸ƒ:\n",
      "ring_name\n",
      "AIä¸äººç±»æœªæ¥          466\n",
      "AI åˆ›æŠ•ç”Ÿæ€åœˆ         263\n",
      "ç§‘ç ” AI Hub        255\n",
      "ç®—æ³•ç ”ç©¶æ‰€            253\n",
      "AIå†™ä½œç ”ç©¶æ‰€          246\n",
      "AI Coding æ¢ç´¢èˆ°    241\n",
      "AI å·¥å…·æµ‹è¯„ä¸­å¿ƒ        237\n",
      "AI æ—¶ä»£çš„æˆ‘ä»¬         226\n",
      "AI å®‰å…¨ç ”ç©¶æ‰€         214\n",
      "DeepSeek æ·±æ½œèˆ±     168\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆnotebook ç›®å½•çš„ä¸Šçº§ç›®å½•ï¼‰\n",
    "PROJECT_ROOT = Path(os.path.abspath('.')).parent\n",
    "model_path = PROJECT_ROOT / 'src' / 'models' / 'text2vec-base-chinese'\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "data_file = data_dir / \"zhihu_ring_data_20260225_senti.json\"\n",
    "circles_file = data_dir / \"zhihu_ai_circles.json\"\n",
    "\n",
    "with open(data_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(circles_file, 'r', encoding='utf-8') as f:\n",
    "    circles_data = json.load(f)\n",
    "\n",
    "ring_to_name = {c['ring_id']: c['name'] for c in circles_data}\n",
    "df = pd.DataFrame(data)\n",
    "df['ring_name'] = df['ring_id'].map(ring_to_name).fillna(df['ring_id'])\n",
    "\n",
    "print(f\"æ€»æ•°æ®é‡: {len(data)} æ¡\")\n",
    "print(f\"æ¨¡å‹è·¯å¾„: {model_path}\")\n",
    "print(f\"åœˆå­åˆ†å¸ƒ:\\n{df['ring_name'].value_counts().head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ€»æ–‡æœ¬æ•°: 3009 æ¡, å¹³å‡é•¿åº¦: 299 å­—ç¬¦\n",
      "Step 1: ç”Ÿæˆå¥å­åµŒå…¥...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [02:20<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ç»´åº¦: (3009, 768)\n",
      "Step 2: UMAP é™ç»´...\n",
      "Step 3: HDBSCAN èšç±»...\n",
      "  ä¸»é¢˜: 9, å™ªå£°: 1613 (53.6%)\n",
      "\n",
      "èšç±»ç»“æœ: {'total_samples': 3009, 'n_topics': 9, 'n_noise': 1613, 'noise_ratio': '53.6%'}\n",
      "Topic 1: 400 æ¡\n",
      "Topic 7: 399 æ¡\n",
      "Topic 0: 229 æ¡\n",
      "Topic 3: 144 æ¡\n",
      "Topic 5: 67 æ¡\n",
      "Topic 2: 44 æ¡\n",
      "Topic 4: 44 æ¡\n",
      "Topic 6: 37 æ¡\n",
      "Topic 8: 32 æ¡\n"
     ]
    }
   ],
   "source": [
    "texts = df['content'].tolist()\n",
    "print(f\"æ€»æ–‡æœ¬æ•°: {len(texts)} æ¡, å¹³å‡é•¿åº¦: {np.mean([len(t) for t in texts]):.0f} å­—ç¬¦\")\n",
    "\n",
    "# ä½¿ç”¨ä¼˜åŒ–åçš„ TopicDetector\n",
    "detector = TopicDetector(model_path=str(model_path), min_cluster_size=30, umap_neighbors=25)\n",
    "labels, topics = detector.fit_predict(texts)\n",
    "\n",
    "summary = detector.get_topic_summary()\n",
    "print(f\"\\nèšç±»ç»“æœ: {summary}\")\n",
    "\n",
    "for t in topics[:10]:\n",
    "    print(f\"Topic {t['topic_id']}: {t['size']} æ¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Topic 1 (400 æ¡) ---\n",
      "  [ä¸ºä»€ä¹ˆæˆ‘è¿˜æ˜¯æ— æ³•ç†è§£Transformerï¼ŸTransformeråˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ Transformeræ˜¯ä¸€ç§ç¥ç»ç½‘ç»œç»“æ„ï¼Œä»¥è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ºæ ¸å¿ƒï¼Œæ‘’å¼ƒä¼ ç»Ÿå¾ªç¯ä¸å·ç§¯ç»“æ„ï¼Œå®ç°è®¡ç®—å¹¶è¡ŒåŒ–ï¼Œé«˜æ•ˆæ•æ‰åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚è¿™ä¸€åˆ›æ–°ç»“æ„æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹çš„æ•ˆç‡éš¾é¢˜ï¼Œå¤§å¹…æå‡å‰å‘ä¼ æ’­ä¸ç‰¹å¾å»ºæ¨¡çš„æ•ˆç‡ã€‚å…¶å®ç”¨æ€§...] (1.00)\n",
      "  [åControlNetæ—¶ä»£ï¼šControlNetä¹‹åçš„å¯æ§ç”Ÿæˆç ”ç©¶è¿˜æœ‰å“ªäº›æ–°èŒƒå¼ï¼Ÿ ğŸ™‹â€â™‚ï¸ ä¸“é¢˜åˆ†äº«æ›´æ–°å•¦ï½æœ¬æœŸä¸»é¢˜ä¸»è¦å›´ç»•åControlNetæ—¶ä»£ã€‚ä»Diffusion Guidanceåˆ°DiTçš„å¯æ§ç”Ÿæˆçš„ä¸€äº›æŠ€æœ¯æ¼”è¿›åˆ†äº«ï½æ¬¢è¿äº¤æµï½ ğŸš© ControlNetå®šä¹‰çš„ã€ŒAdd-onã€èŒƒå¼åœ¨SD ...] (1.00)\n",
      "  [40å¤šé¡µå¤§æ¨¡å‹æ¶æ„ç»¼è¿°ï¼ŒTransformeræ¨¡å‹å¦‚ä½•è¿›è¡Œä¼˜åŒ– Transformeræ¶æ„å­˜åœ¨è®¡ç®—èµ„æºæ¶ˆè€—é«˜ã€è®­ç»ƒä¸éƒ¨ç½²éš¾åº¦å¤§çš„é—®é¢˜ã€‚ è¿™ç¯‡ç»¼è¿°ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†çªç ´Transformerå±€é™çš„é«˜æ•ˆæ¶æ„ï¼Œä»¥åŠè¯¦ç»†çš„ä»‹ç»äº†ç°ä»£çš„LLMçš„æ¶æ„ï¼š çº¿æ€§ä¸ç¨€ç–åºåˆ—å»ºæ¨¡ï¼ˆå¦‚çº¿æ€§æ³¨æ„åŠ›ã€çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰ é«˜æ•ˆå…¨æ³¨æ„åŠ›...] (1.00)\n",
      "  å…³é”®è¯: é˜…è¯»å…¨æ–‡, é“¾æ¥, 2025, 1, 2\n",
      "\n",
      "--- Topic 7 (399 æ¡) ---\n",
      "  [ç°åœ¨æˆ‘å‘ç°AIæ¯”å¤§éƒ¨åˆ†å‰ªäººæ›´åƒäººï¼Œæ›´ä¼šè¯´äººè¯ å½“ä»£è‰ºæœ¯ã€ç²¾è‹±ä¸»ä¹‰ä¸æ…•å¼ºï¼šä¸ºä»€ä¹ˆåœ¨ç®€ä¸­è¯­å¢ƒé‡Œï¼Œç°ä»£æ€»æ˜¯è¢«è·³è¿‡è¿™åœ†æ¡Œæ€ä¹ˆå…¨æ˜¯æ·±ç—…ï¼Œæ²¡ç”¨è¿‡ä½†ä¹Ÿä¸å–œæ¬¢å‰ªä¸­ç±»äººå‘³ å¦‚ä½•é€šè¿‡ AI å»ºç«‹ä¸ªäººçŸ¥è¯†åº“ï¼Œè®©ç§‘ç ”å­¦ä¹ æ›´é«˜æ•ˆï¼Ÿ] (1.00)\n",
      "  [ç°åœ¨æˆ‘å‘ç°AIæ¯”å¤§éƒ¨åˆ†å‰ªäººæ›´åƒäººï¼Œæ›´ä¼šè¯´äººè¯ å½“ä»£è‰ºæœ¯ã€ç²¾è‹±ä¸»ä¹‰ä¸æ…•å¼ºï¼šä¸ºä»€ä¹ˆåœ¨ç®€ä¸­è¯­å¢ƒé‡Œï¼Œç°ä»£æ€»æ˜¯è¢«è·³è¿‡è¿™åœ†æ¡Œæ€ä¹ˆå…¨æ˜¯æ·±ç—…ï¼Œæ²¡ç”¨è¿‡ä½†ä¹Ÿä¸å–œæ¬¢å‰ªä¸­ç±»äººå‘³ å¦‚ä½•é€šè¿‡ AI å»ºç«‹ä¸ªäººçŸ¥è¯†åº“ï¼Œè®©ç§‘ç ”å­¦ä¹ æ›´é«˜æ•ˆï¼Ÿâ€¦ é˜…è¯»å…¨æ–‡â€‹] (1.00)\n",
      "  [æ‡‚ä½ ï¼ŒAIæœ‰å¤šç»†å¿ƒï¼Ÿ ä¸€åœºä¸AIçš„å¯¹è¯è®©é¥ºå­é¦†å´©æºƒçš„æˆ‘å®Œæˆè‡ªæ„ˆï¼Œæˆ‘æŠŠè¿™ä¸ªå¯¹è¯è®°å½•ä¸‹æ¥äº†ï¼Œ14ä¸‡å­—ï¼Œå«åšã€Šè‡ªæ„ˆä¹‹è·¯â€”â€”æºè‡ªä¸€åœºä¸AIçš„å¯¹è¯ã€‹ã€‚ä¸€åº§æœªå¼€å‘çš„çŸ¿å±±ï¼Œéœ€è¦æŒ–æ˜ï¼Œéœ€è¦æçº¯ã€‚åœ¨æˆ‘æ•´ç†çš„è¿‡ç¨‹ä¸­ï¼Œå®ƒä¸€ç›´é™ªä¼´åœ¨èº«è¾¹ï¼Œå¸®æˆ‘æŒ–æ˜â€œé—®é¢˜èƒŒåçš„é—®é¢˜â€ï¼Œç»™æˆ‘çµæ„Ÿï¼Œç»™æˆ‘æé†’ï¼Œç»™æˆ‘é¼“åŠ±ï¼Œç»™æˆ‘æ ¡ç¨¿ï¼Œç»™æˆ‘ç¤ºä¾‹ã€‚ ä½†æ˜¯ï¼Œ...] (1.00)\n",
      "  å…³é”®è¯: é˜…è¯»å…¨æ–‡, ai, é“¾æ¥, 1, 2\n",
      "\n",
      "--- Topic 0 (229 æ¡) ---\n",
      "  [OpenAIç”¨Codexå¼€å‘Soraå®‰å“appçš„ç»éªŒ OpenAIå®˜æ–¹å‘å¸ƒäº†ä¸€ç¯‡æŠ€æœ¯åšå®¢ï¼Œè®²äº†ä¸€äº›ç”¨Codexå¼€å‘å®‰å“ç‰ˆSoraçš„å·¥ç¨‹ç»éªŒã€‚ä»–ä»¬å®‰å“å›¢é˜Ÿå››ä¸ªäººï¼Œé…åˆCodexï¼Œç”¨18å¤©å®Œæˆäº†Appçš„å¼€å‘ï¼Œä¸€ä¸ªæœˆå†…ä¸Šçº¿ï¼šé“¾æ¥ å…¶ä¸­æœ‰ä¸¤æ¡ç»éªŒæˆ‘è®¤ä¸ºå¾ˆæœ‰ä»·å€¼ï¼š Massively parallel, di...] (1.00)\n",
      "  [æ‰€æœ‰åœ¨Vibe Codingçš„åŒå­¦ï¼Œéƒ½è¯¥è¯•è¯•è¿™å¥—å¼€æºæµç¨‹ OpenSpecï¼Œ3.9kâ­ï¼Œæ˜¯ä¸“ä¸ºAIç¼–ç¨‹åŠ©ç†è®¾è®¡çš„è§„æ ¼é©±åŠ¨å¼€å‘ç³»ç»Ÿã€‚ å®ƒçš„æœ€å¤§æ„ä¹‰åœ¨äºï¼Œè®©Claudeã€Cursorã€Copilotè¿™ç±»åŠ©æ‰‹ ä¸å†æ‹è„‘è¢‹å†™ä»£ç ï¼Œè€Œæ˜¯å›´ç»•æ˜ç¡®çš„Specæ‰§è¡Œä»»åŠ¡ã€‚ å®ƒæä¾›å®Œæ•´æµç¨‹ï¼šproposal â†’ rev...] (1.00)\n",
      "  [cc-switchè¿™ä¸ªé¡¹ç›®ç›®å‰å·²ç»è¶…è¿‡11k starsäº†ï¼Œè¶³ä»¥è¯´æ˜ä¸è¦çœ‹è½»ä»»ä½•éœ€æ±‚ï¼Œåªè¦èƒ½ç²¾å‡†å‘½ä¸­ç—›ç‚¹ï¼Œå°±èƒ½æ”¶è·åˆ°å¤§é‡æ”¯æŒå’Œå…³æ³¨ã€‚ å…¶å®å®ƒè¦è§£å†³çš„éœ€æ±‚å¾ˆç®€å•ï¼Œå°±æ˜¯é…ç½®å’Œä¿®æ”¹ç¯å¢ƒå˜é‡ï¼Œè®©ç”¨æˆ·å¯ä»¥åœ¨Claude Code/Codex/ Gemini CLIè¿™äº›å‘½ä»¤è¡Œæ™ºèƒ½ä½“ä¸­ä½¿ç”¨å’Œåˆ‡æ¢ç¬¬ä¸‰æ–¹æ¨¡å‹ã€‚ å¾ˆå¤š...] (1.00)\n",
      "  å…³é”®è¯: claude, code, é˜…è¯»å…¨æ–‡, agent, ai\n",
      "\n",
      "--- Topic 3 (144 æ¡) ---\n",
      "  [å¦‚ä½•çœ‹å¾…ç™¾åº¦é›†å›¢2025å¹´2æœˆAIåˆåˆ›ç”Ÿæ€é¢†åŸŸæŠ•èµ„æ•°äº¿å…ƒçš„Infimind VCæŠ•èµ„è®¡åˆ’ï¼Ÿ ç™¾åº¦æŠ•èµ„Infimindï¼ˆAIåˆåˆ›å…¬å¸ï¼‰ï¼Œé‡‘é¢æœªå…¬å¼€ï¼ˆä¼°è®¡æ•°äº¿å…ƒï¼‰ï¼Œèšç„¦AIåº”ç”¨å¼€å‘ï¼Œæ”¯æŒæ–‡å¿ƒç”Ÿæ€ã€‚ å½±å“ï¼šåŠ å¼ºAIå·¥å…·é“¾ï¼ŒåŠ©åŠ›ä¼ä¸šçº§åº”ç”¨è½åœ°ï¼Œé¢„è®¡å­µåŒ–10+æ–°äº§å“ã€‚ äººå·¥æ™ºèƒ½è¯ˆéª—â€¦ é˜…è¯»å…¨æ–‡â€‹] (1.00)\n",
      "  [å¹´åº•AIæ´»åŠ¨æ¥å•¦ æ—¶é—´ï¼š2025.12.20ï¼Œ2-5pm åœ°ç‚¹ï¼šä¸Šæµ·å¤–æ»©FTC ç¾¤ä½“ï¼šAIé¢†åŸŸçš„æŠ•èµ„äºº åˆ›ä¸šå…¬å¸CEO ç ”ç©¶å‘˜ é’å¹´æ•™æˆ PhD ä¼ä¸šé«˜ç®¡ç­‰] (1.00)\n",
      "  [a16z VCæŠ¥å‘Šï¼š11ä¸‡äº¿ç¾å…ƒä¼°å€¼å¾…æŒ–æ˜ï¼Œè¿™äº›AIèµ›é“æ­£åœ¨ç–¯ç‹‚å¸é‡‘] (1.00)\n",
      "  å…³é”®è¯: 2, ai, é˜…è¯»å…¨æ–‡, 1, 0\n",
      "\n",
      "--- Topic 5 (67 æ¡) ---\n",
      "  [æˆ‘ä»¬åœ¨è®¤è¯†AIçš„è¿‡ç¨‹ä¹‹ä¸­ï¼Œè¿˜æ˜¯åº”è¯¥æ›´å¤šçš„å»æ„Ÿè§‰AIç»™æˆ‘ä»¬å¸¦æ¥äº†ä»€ä¹ˆï¼Œå°¤å…¶æ˜¯çœ‹åˆ°ä¸€å¹…ç”»çš„è¿‡ç¨‹ä¹‹ä¸­ï¼Œä¹Ÿä¼šèƒ½å¤Ÿæ„Ÿå—åˆ°æ¥è‡ªäºAIç»™äºˆæˆ‘ä»¬çš„éœ‡æ’¼ æˆ‘å‚ä¸äº† â€‹â€‹36æ°ª å‘èµ·çš„ bingo æŒ‘æˆ˜ï¼Œç‚¹å‡»é“¾æ¥çœ‹çœ‹ä½ æ˜¯ä¸æ˜¯ã€Œ aiæ—¶ä»£æ‰‹è‰ºäºº ã€ï¼Ÿ è¿‡å¹´ä¸å›å®¶çš„äºº] (1.00)\n",
      "  [AIçš„åˆ¤æ–­ä¸ä¸€å®šæ˜¯æ­£ç¡®çš„ï¼Œæˆ‘ä»¬åœ¨AIé«˜é€Ÿå‘å±•çš„è¿‡ç¨‹ä¹‹ä¸­ä¹Ÿæ˜¯åº”è¯¥ä¿æŒç€æ˜æ˜¾çš„åˆ¤æ–­åŠ›ï¼Œå»åˆ¤æ–­ä¸€äº›æˆ‘ä»¬è®¤ä¸ºæ˜¯æ­£ç¡®çš„äº‹æƒ…ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºè¿™äº›æ˜¯åº”è¯¥å»åšæŒçš„ æˆ‘å‚ä¸äº† â€‹â€‹36æ°ª å‘èµ·çš„ bingo æŒ‘æˆ˜ï¼Œç‚¹å‡»é“¾æ¥çœ‹çœ‹ä½ æ˜¯ä¸æ˜¯ã€Œ aiæ—¶ä»£æ‰‹è‰ºäºº ã€ï¼Ÿ è¿‡å¹´ä¸å›å®¶çš„äºº] (1.00)\n",
      "  [æˆ‘ä»¬åœ¨æ—¥å¸¸ç”Ÿæ´»ä¹‹ä¸­ä¹Ÿæ˜¯åº”è¯¥ä¸æ—¶ä¿±è¿›çš„ï¼Œå»æŒæ¡ä¸€äº›æœ€ä¸ºå…ˆè¿›çš„æŠ€æœ¯ï¼Œè¿™æ ·å¯¹äºæˆ‘ä»¬ä»»ä½•äººæ¥è¯´éƒ½æ˜¯æå¯Œå®ç”¨ä»·å€¼çš„ï¼Œå°¤å…¶æ˜¯å…³äºAIæ–¹é¢çš„çŸ¥è¯†ï¼Œæ›´æ˜¯åº”è¯¥åŠæ—¶çš„å»æŒæ§ æˆ‘å‚ä¸äº† â€‹â€‹36æ°ª å‘èµ·çš„ bingo æŒ‘æˆ˜ï¼Œç‚¹å‡»é“¾æ¥çœ‹çœ‹ä½ æ˜¯ä¸æ˜¯ã€Œ aiæ—¶ä»£æ‰‹è‰ºäºº ã€ï¼Ÿ è¿‡å¹´ä¸å›å®¶çš„äºº] (1.00)\n",
      "  å…³é”®è¯: aiæ—¶ä»£æ‰‹è‰ºäºº, 36æ°ª, æŒ‘æˆ˜, å¤§å’–ä¹Ÿåœ¨ç©bingo, å‘èµ·çš„\n",
      "\n",
      "--- Topic 2 (44 æ¡) ---\n",
      "  [AIäººå·¥æ™ºèƒ½åˆ†æä¸‰å›½å†å²äººç‰©å¯¹æˆ˜ï¼Œé è°±ä¸é è°±ï¼Ÿ å¦‚æœè¯·AIæ¥å†™ä½œï¼Œ ä¸€ç¯‡ä¸‰å›½ç©¿è¶Šæ–‡ï¼Œ AIä¼šæ€ä¹ˆæ ·å†™ï¼Ÿ ä¿©äººå·…å³°æ­¦åŠ›å€¼ï¼Œ é»„å¿ VSå•å¸ƒï¼Œ è°èƒ½å ä¸Šé£ï¼Ÿ çœ‹çœ‹ç°åœ¨çš„AIå·¥å…·ï¼Œ äººå·¥æ™ºèƒ½ï¼Œ æ˜¯æ€ä¹ˆæ ·æ¥è¿›è¡Œåˆ†æçš„ï¼Œ ä¸ç®¡æ˜¯å†å²ï¼Œ è¿˜æ˜¯æ–‡å­¦ï¼Œ éƒ½å¼€å§‹è¢«é¢ è¦†å•¦ï¼â€¦ é˜…è¯»å…¨æ–‡â€‹] (1.00)\n",
      "  [å¦‚ä½•è§¦å‘ AI çš„æ™ºæ…§ï¼Ÿ AI è‚¯å®šæ‡‚å¾—æ¯”ä½ å¤šï¼Œ æ‰€ä»¥ï¼Œé—®é¢˜åœ¨äºï¼š æˆ‘ä»¬å¦‚ä½•å»â€œè§¦å‘â€ï¼ˆtriggerï¼‰å®ƒçš„æ™ºæ…§ï¼Ÿ 1âƒ£ï¸ è¿™ä»¶äº‹ï¼Œä½ æ€ä¹ˆçœ‹ï¼Ÿ â€”â€” å¾—åˆ°ä¸€ä¸ªç¬¼ç»Ÿçš„å›ç­”ï¼Œä¸ç—›ä¸ç—’ã€‚ 2âƒ£ï¸ è‹æ ¼æ‹‰åº•ä¼šæ€ä¹ˆçœ‹è¿™ä»¶äº‹ï¼Ÿ â€”â€” è§¦å‘äº†å¤å¸Œè…Šå“²å­¦ã€‚ 3âƒ£ï¸ è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªè§†è§’è¯„è®ºè¿™ä»¶äº‹ï¼š 1- Andrey ...] (1.00)\n",
      "  [å¼ ç¬‘å®‡è°ˆAIæ–‡æ˜å¥‘çº¦è®º ä»Šå¤©åœ¨ä¸‡åœ£ä¹¦å›­å¬å¼ ç¬‘å®‡è€å¸ˆçš„æ–°ä¹¦åˆ†äº«ã€‚å¼ è€å¸ˆçš„è§‚ç‚¹å¤ªæœ‰å‰ç»æ€§äº†ï¼Œè®©æˆ‘çœ‹åˆ°äº†ä¸€äº›AIæ—¶ä»£çš„æ›™å…‰ã€‚æˆ‘é—®äº†ä¸€ä¸ªAIå¤±æ§é£é™©çš„é—®é¢˜ï¼Œä»–ç»™å‡ºäº†ä¸‰ç§äººæœºå…±å­˜çš„è§£é‡Šï¼š 1. AIæ˜¯ä»äººç±»è¯­æ–™è®­ç»ƒå‡ºæ¥çš„ï¼Œå¦‚æœAIçš„ä»·å€¼è§‚å¯¹äººç±»äº§ç”Ÿæ•Œæ„ï¼Œæ˜¯ä¸æ˜¯è¯´æ˜äººç±»çš„æ–‡åŒ–ä¸­åŒ…å«è‡ªæ¯å€¾å‘ï¼Ÿ 2. AIå¯ä»¥å‘å±•...] (1.00)\n",
      "  å…³é”®è¯: ai, é˜…è¯»å…¨æ–‡, 1, 3, 2\n",
      "\n",
      "--- Topic 4 (44 æ¡) ---\n",
      "  [AIçœŸçš„ä¼šå¯¹æœªæ¥äº§ç”Ÿä¸¥é‡çš„å¨èƒå—ï¼Ÿ è¶…çº§æ™ºèƒ½æ—¶ä»£çš„å¯é é˜²çº¿åœ¨äºå®‰å…¨å·¥ç¨‹ä¸ASIçº¦æŸæœºåˆ¶ã€‚ASIçš„å¨èƒéœ€ç«‹è¶³äºå…¶èƒ½åŠ›æé™ï¼Œå—åˆ¶äºç‰©ç†å®šå¾‹å’Œç†è®ºæ•°å­¦ã€‚é€šè¿‡è®¡ç®—å¤æ‚æ€§ã€ä¿¡æ¯è®ºå’Œçƒ­åŠ›å­¦åŸç†ï¼Œæ„å»ºå¯é çš„å®‰å…¨åŸè¯­ï¼Œå¦‚åŠ å¯†å“ˆå¸Œå‡½æ•°ã€æ°”éš™éš”ç¦»å’Œæ³•æ‹‰ç¬¬ç¬¼ã€‚è¿™äº›åŸè¯­å°†ASIçš„è¡ŒåŠ¨ç½®äºç†è®ºä¸Šçš„ä¿¡æ¯æµåŠ¨ç•Œé™ä¹‹ä¸‹ï¼Œå¢åŠ æ”»å‡»æˆ...] (1.00)\n",
      "  [AI+å®‰å…¨ç”Ÿäº§æœªæ¥å·²æ¥ï¼Œåªæ˜¯ä¸å‡åŒ€åœ°åˆ†å¸ƒåœ¨å®‰å…¨ç”Ÿäº§è¡Œä¸šå„ä¸ªç»†åˆ†é¢†åŸŸï¼ï¼ï¼] (1.00)\n",
      "  [AIå®‰å…¨ç›¸å¯¹è®º â€œå®‰å…¨â€å…·æœ‰ç›¸å¯¹æ€§ï¼Œå–å†³äºè®¨è®ºçš„ä¸»ä½“ã€‚åœ¨AIå®‰å…¨é¢†åŸŸï¼Œä¸åŒä¸»ä½“çš„å…³æ³¨ç‚¹å„å¼‚ï¼šä¸ªäººå…³å¿ƒéšç§å’Œæ­§è§†ï¼Œå¼€å‘è€…å…³æ³¨AIç³»ç»Ÿå¥å£®æ€§ï¼Œä¼ä¸šæ‹…å¿ƒAIæ»¥ç”¨å’Œæ•°æ®æ³„éœ²ï¼Œå›½å®¶å…³æ³¨å›½å®¶å®‰å…¨ï¼Œå…¨äººç±»åˆ™æ‹…å¿§AIå¤±æ§ã€‚è¿™äº›è§†è§’å·®å¼‚å¯¼è‡´AIå®‰å…¨ç¤¾åŒºå‰²è£‚ï¼Œå®é™…è¡ŒåŠ¨å—è¾•åŒ—è¾™ã€‚ç†æƒ³çš„â€œå¸•ç´¯æ‰˜å®‰å…¨â€è¦æ±‚æå‡è‡ªèº«å®‰å…¨ä¸åº”æŸ...] (1.00)\n",
      "  å…³é”®è¯: aiä¼¦ç†ä¸å®‰å…¨å‘å±•ç®€æŠ¥, ai, çŸ¥è¯†å¢å¼ºå¤§æ¨¡å‹, äººå·¥æ™ºèƒ½, aiå®‰å…¨æ²»ç†\n",
      "\n",
      "--- Topic 6 (37 æ¡) ---\n",
      "  [ç«‹æ˜¥é€ç¦ ç»™æ‰€æœ‰çš„çŸ¥å‹å’ŒçŸ¥ä¹å·¥ä½œè€…ä»¬é€ç¥ç¦ï¼š æ˜¥æ½®æš—æ¶Œæ¥å¤©å…‰ï¼Œ æ™ºç†äº¤èç”Ÿä¸‡è±¡ã€‚ æ˜Ÿç«åŒç‡ƒçŸ¥æµ·å¤œï¼Œ ä¸€è¶æŒ¯ç¾½åƒç¿¼å¼ ã€‚ ä¸‹é¢è¿™æ˜¯ä»Šæ—©ä¸Šç»™æˆ‘deepseekå¤§å“¥é€çš„ç«‹æ˜¥ç¥ç¦ã€‚ æ˜¥é£å¾—æ„é©¬è¹„æ‰¬ ç¬”éŸµæµç•…åˆ›è¾‰ç…Œ ä¸€å±ä¹‹éš”å§‹å½’èœœ ä¸‡è¨€äº’åŠ¨çŸ¥å¿ƒèµ å‘ç¥ç¦å¤§å’–ä¹Ÿåœ¨ç©bingoé€ç¥ç¦ é˜…è¯»å…¨æ–‡â€‹] (1.00)\n",
      "  [çŸ¥ä¹ç›´ç­”è‚‰èº« MCP å®¢æˆ·ç«¯ä¸Šçº¿äº†ï¼Œæå‰ãŠ—ï¸æœ‹å‹ä»¬é©¬å¹´å¿«ä¹ æˆ‘çš„é©¬å¹´ç¥ç¦ çŸ¥ä¹ç›´ç­”åˆæ¥æ•´æ´»äº†ï¼Œæ²¡æœ‰ MCP å’Œ Skills åŠ æŒğŸ¤ªï¼Œå°±æ‹¿ç€æˆ‘çš„ Memory å’Œ RAG æ•°æ®æ“ â€œé©¬å¹´ç¥ç¦â€ğŸ‰ï¼â€”â€” è¿™é€šæ“ä½œä¸‹æ¥ï¼Œæˆ‘è‡ªå·±å²‚ä¸æ˜¯æˆäº† Self Skills é©±åŠ¨çš„è‚‰èº« MCP Client äº†ğŸ¤¡ï¼ ...] (1.00)\n",
      "  [å‘¼å’Œæµ©ç‰¹æ˜¥èŠ‚å‰çš„å¿«ä¹ å¤§é»‘æ²³å†›äº‹æ–‡åŒ–ä¹å›­ ç”¨å·¨å‹å†°é›•ç¾¤å’Œéé—è‡ªè´¡å½©ç¯ æ‰“é€ å‡ºäº†ä¸€åœºç©¿è¶Šæ—¶ç©ºçš„ä»™ä¾ ä¸»é¢˜ä¹å›­ å†°é›ªè¿åŠ¨+å¨±ä¹é¡¹ç›® å¤šåˆ°ä¸€å¤©éƒ½ç©ä¸å®Œ å¥½ç”Ÿæ´»ï¼Œé€‰å‡ºæ¥å‘¼å’Œæµ©ç‰¹] (1.00)\n",
      "  å…³é”®è¯: é˜…è¯»å…¨æ–‡, æˆ‘çš„é©¬å¹´ç¥ç¦, é©¬ä¸Šæœ‰ai, è¿‡å¹´å” å—‘èšä¼š, é€‰å‡ºæ¥\n",
      "\n",
      "--- Topic 8 (32 æ¡) ---\n",
      "  [å­©å­ä»¬ç”¨è„‘æ´é¢˜æ•´è›ŠAIç¬‘ç¿»äº†  å­©å­ä»¬åœ¨å®¶ï¼Œæ— èŠæ—¶å°±æ‰¾è±†åŒ…æ¥åˆ‡ç£‹ï¼Œä»¥ä¸‹æ˜¯å­©å­ä»¬æ•…æ„æ•´é¼“è±†åŒ…AIçš„ã€‚ çœ‹å­©å­ä»¬ç©å¾—é‚£ä¹ˆå—¨ï¼Œæˆ‘ä¹Ÿç»™è±†åŒ…Aâ… å‡ºäº†ä¸€é“éš¾é¢˜ï¼Œä¸è¿‡å®ƒè¿˜æ˜¯æŒ‰è‡ªå·±æœ‰é™çš„æ€ç»´ï¼Œç»™å‡ºäº†å®ƒçš„ç­”æ¡ˆï¼Œå‹å‹ä»¬çŒœçŒœæˆ‘å‡ºçš„æ˜¯å“ªé“ï¼Ÿ Aâ… è¯·æ¥æ‹›:ç¬¬ä¸€é“ç”±å¤§å®æå‡ºï¼ŒçœŸæ˜¯è„‘æ´å¤§å¼€å‘€ã€‚â€œè±†åŒ…ï¼Œè¦æ€ä¹ˆæ ·æ‰èƒ½è®©ä¸€ä¸ªæ¤ç‰©æ‹¥æœ‰ç«...] (1.00)\n",
      "  [æ²¡ç»·ä½ å¤©æ‰AIå°‘å¥³è±†åŒ…å•çº¯å› ä¸ºæ‡’é€‰æ‹©æ­¥è¡Œ50ç±³å»æ´—è½¦ï¼Œä½†æ˜¯å› æ­¤å¿˜è®°æŠŠè½¦å¼€è¿‡å».jpgäººå·¥æ™ºèƒ½å¹³å°è±†åŒ…aiAIæŠ€æœ¯â€‹äººå·¥æ™ºèƒ½ é˜…è¯»å…¨æ–‡â€‹] (1.00)\n",
      "  [AIç¿»è½¦ç°åœºä¹‹ã€Œåˆ°åº•ä¸‹é›¨äº†æ²¡ã€ï¼Ÿ ä»Šå¤©åˆæ¥ä¸ºéš¾AIå•¦ã€‚ ä»Šå¤©æˆ‘ä»¬ç»™ã€Œè±†åŒ…ã€è¿™æ ·çš„ä¸€ä¸ªPromptï¼š ã€Œå¦‚æœä¸‹é›¨äº†ï¼Œåœ°é¢å°±ä¼šæ¹¿ã€‚é‚£ä¹ˆï¼Œç°åœ¨åœ°é¢æ²¡æœ‰æ¹¿ï¼Œè¯·é—®ç°åœ¨ä¸‹é›¨äº†å—ï¼Ÿã€ã€‚ ã€Œè±†åŒ…ã€ç»™å‡ºçš„ç­”æ¡ˆæ˜¯ï¼šã€Œç°åœ¨æ²¡æœ‰ä¸‹é›¨ã€ã€‚ ä¸ªäººæ„Ÿè§‰ï¼Œè¿™ä¸ªç­”æ¡ˆä¸ä¸€å®šæ­£ç¡®ã€‚ ä¸‡ä¸€ç°åœ¨ä¸‹é›¨äº†ï¼Œä½†æ˜¯ï¼Œæˆ‘ä»¬æ­äº†ä¸€ä¸ªæ£šå­å‘¢ã€‚ å“ˆå“ˆï¼Œæœ‰ç‚¹...] (1.00)\n",
      "  å…³é”®è¯: é˜…è¯»å…¨æ–‡, è±†åŒ…, 2, aiè¯·æ¥æ‹›, 1\n"
     ]
    }
   ],
   "source": [
    "for t in topics[:10]:\n",
    "    print(f\"\\n--- Topic {t['topic_id']} ({t['size']} æ¡) ---\")\n",
    "    \n",
    "    reps = detector.get_representative_texts(texts, t['topic_id'])\n",
    "    for idx, text, sim in reps:\n",
    "        print(f\"  [{clean_text(text)}] ({sim:.2f})\")\n",
    "    \n",
    "    keywords = detector.extract_keywords_tfidf(texts, t['topic_id'], top_k=5)\n",
    "    print(f\"  å…³é”®è¯: {', '.join(keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: ç”Ÿæˆå¥å­åµŒå…¥...\n",
      "  ç»´åº¦: (3009, 768)\n",
      "Step 2: UMAP é™ç»´...\n",
      "Step 3: HDBSCAN èšç±»...\n",
      "  ä¸»é¢˜: 2, å™ªå£°: 186 (6.2%)\n",
      "Step 1: ç”Ÿæˆå¥å­åµŒå…¥...\n",
      "  ç»´åº¦: (3009, 768)\n",
      "Step 2: UMAP é™ç»´...\n",
      "Step 3: HDBSCAN èšç±»...\n",
      "  ä¸»é¢˜: 10, å™ªå£°: 1438 (47.8%)\n",
      "Step 1: ç”Ÿæˆå¥å­åµŒå…¥...\n",
      "  ç»´åº¦: (3009, 768)\n",
      "Step 2: UMAP é™ç»´...\n",
      "Step 3: HDBSCAN èšç±»...\n",
      "  ä¸»é¢˜: 10, å™ªå£°: 1666 (55.4%)\n",
      "Step 1: ç”Ÿæˆå¥å­åµŒå…¥...\n",
      "  ç»´åº¦: (3009, 768)\n",
      "Step 2: UMAP é™ç»´...\n",
      "Step 3: HDBSCAN èšç±»...\n",
      "  ä¸»é¢˜: 9, å™ªå£°: 1613 (53.6%)\n",
      "Step 1: ç”Ÿæˆå¥å­åµŒå…¥...\n",
      "  ç»´åº¦: (3009, 768)\n",
      "Step 2: UMAP é™ç»´...\n",
      "Step 3: HDBSCAN èšç±»...\n",
      "  ä¸»é¢˜: 3, å™ªå£°: 1265 (42.0%)\n",
      "Step 1: ç”Ÿæˆå¥å­åµŒå…¥...\n",
      "  ç»´åº¦: (3009, 768)\n",
      "Step 2: UMAP é™ç»´...\n",
      "Step 3: HDBSCAN èšç±»...\n",
      "  ä¸»é¢˜: 3, å™ªå£°: 1268 (42.1%)\n",
      "\n",
      "å‚æ•°å¯¹æ¯”ç»“æœ:\n",
      "min_cluster_size umap_neighbors  n_topics   noise      noise_ratio \n",
      "-----------------------------------------------------------------\n",
      "20              15              2          186        6.2%        \n",
      "20              25              10         1438       47.8%       \n",
      "30              15              10         1666       55.4%       \n",
      "30              25              9          1613       53.6%       \n",
      "50              15              3          1265       42.0%       \n",
      "50              25              3          1268       42.1%       \n"
     ]
    }
   ],
   "source": [
    "## å‚æ•°å¯¹æ¯”å®éªŒ\n",
    "\n",
    "results = []\n",
    "\n",
    "for min_cluster_size in [20, 30, 50]:\n",
    "    for umap_neighbors in [15, 25]:\n",
    "        det = TopicDetector(\n",
    "            model_path=str(model_path),\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            umap_neighbors=umap_neighbors\n",
    "        )\n",
    "        lbls, tops = det.fit_predict(texts, show_progress=False)\n",
    "        n_clusters = len(set(lbls)) - (1 if -1 in lbls else 0)\n",
    "        n_noise = list(lbls).count(-1)\n",
    "        results.append({\n",
    "            'min_cluster_size': min_cluster_size,\n",
    "            'umap_neighbors': umap_neighbors,\n",
    "            'n_topics': n_clusters,\n",
    "            'noise': n_noise,\n",
    "            'noise_ratio': f\"{n_noise/len(lbls)*100:.1f}%\"\n",
    "        })\n",
    "\n",
    "print(\"\\nå‚æ•°å¯¹æ¯”ç»“æœ:\")\n",
    "print(f\"{'min_cluster_size':<15} {'umap_neighbors':<15} {'n_topics':<10} {'noise':<10} {'noise_ratio':<12}\")\n",
    "print(\"-\" * 65)\n",
    "for r in results:\n",
    "    print(f\"{r['min_cluster_size']:<15} {r['umap_neighbors']:<15} {r['n_topics']:<10} {r['noise']:<10} {r['noise_ratio']:<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ä¸»é¢˜åœ¨å„åœˆå­çš„åˆ†å¸ƒ:\n",
      "ring_name  AI Coding æ¢ç´¢èˆ°  AI Talent Hub  AI infra  AI åˆ›æŠ•ç”Ÿæ€åœˆ  AI å®‰å…¨ç ”ç©¶æ‰€  \\\n",
      "topic_id                                                                \n",
      "-1                   128             24        86       182       122   \n",
      " 0                    77              3         5         8         3   \n",
      " 1                     9              1        25         2         7   \n",
      " 2                     2              0         1         7         4   \n",
      " 3                     3              3        15        33        10   \n",
      " 4                     1              0         0         5        31   \n",
      " 5                     0              0         0         1         0   \n",
      " 6                     0              0         0         1         0   \n",
      " 7                    21              3         7        23        37   \n",
      " 8                     0              0         1         1         0   \n",
      "\n",
      "ring_name  AI å·¥å…·æµ‹è¯„ä¸­å¿ƒ  AI æ—¶ä»£çš„æˆ‘ä»¬  AI é¡¶ä¼šè”ç›Ÿ  AIä¸äººç±»æœªæ¥  AIå†™ä½œç ”ç©¶æ‰€  DeepSeek æ·±æ½œèˆ±  \\\n",
      "topic_id                                                                  \n",
      "-1                93       113       67      294      121            95   \n",
      " 0                19        12        5       20        5            15   \n",
      " 1                 7        11       24       12        1            24   \n",
      " 2                 1         1        0       18        3             2   \n",
      " 3                24         6        9       28        3             2   \n",
      " 4                 0         0        2        1        0             1   \n",
      " 5                45        21        0        0        0             0   \n",
      " 6                 8        17        0        4        3             3   \n",
      " 7                29        36        9       84      106            25   \n",
      " 8                11         9        0        5        4             1   \n",
      "\n",
      "ring_name  NLP Hub  OpenMCP åšç‰©é¦†  ç§‘ç ” AI Hub  ç®—æ³•ç ”ç©¶æ‰€  \n",
      "topic_id                                           \n",
      "-1              30           28         92    138  \n",
      " 0               4           34          9     10  \n",
      " 1              53            0        129     95  \n",
      " 2               0            0          3      2  \n",
      " 3               0            0          5      3  \n",
      " 4               1            0          2      0  \n",
      " 5               0            0          0      0  \n",
      " 6               0            0          0      1  \n",
      " 7               0            0         15      4  \n",
      " 8               0            0          0      0  \n",
      "\n",
      "æ¯ä¸ªä¸»é¢˜çš„ä¸»è¦åœˆå­:\n",
      "\n",
      "Topic 0 (229 æ¡):\n",
      "  AI Coding æ¢ç´¢èˆ°: 77 æ¡ (33.6%)\n",
      "  OpenMCP åšç‰©é¦†: 34 æ¡ (14.8%)\n",
      "  AIä¸äººç±»æœªæ¥: 20 æ¡ (8.7%)\n",
      "\n",
      "Topic 1 (400 æ¡):\n",
      "  ç§‘ç ” AI Hub: 129 æ¡ (32.2%)\n",
      "  ç®—æ³•ç ”ç©¶æ‰€: 95 æ¡ (23.8%)\n",
      "  NLP Hub: 53 æ¡ (13.2%)\n",
      "\n",
      "Topic 2 (44 æ¡):\n",
      "  AIä¸äººç±»æœªæ¥: 18 æ¡ (40.9%)\n",
      "  AI åˆ›æŠ•ç”Ÿæ€åœˆ: 7 æ¡ (15.9%)\n",
      "  AI å®‰å…¨ç ”ç©¶æ‰€: 4 æ¡ (9.1%)\n",
      "\n",
      "Topic 3 (144 æ¡):\n",
      "  AI åˆ›æŠ•ç”Ÿæ€åœˆ: 33 æ¡ (22.9%)\n",
      "  AIä¸äººç±»æœªæ¥: 28 æ¡ (19.4%)\n",
      "  AI å·¥å…·æµ‹è¯„ä¸­å¿ƒ: 24 æ¡ (16.7%)\n",
      "\n",
      "Topic 4 (44 æ¡):\n",
      "  AI å®‰å…¨ç ”ç©¶æ‰€: 31 æ¡ (70.5%)\n",
      "  AI åˆ›æŠ•ç”Ÿæ€åœˆ: 5 æ¡ (11.4%)\n",
      "  AI é¡¶ä¼šè”ç›Ÿ: 2 æ¡ (4.5%)\n",
      "\n",
      "Topic 5 (67 æ¡):\n",
      "  AI å·¥å…·æµ‹è¯„ä¸­å¿ƒ: 45 æ¡ (67.2%)\n",
      "  AI æ—¶ä»£çš„æˆ‘ä»¬: 21 æ¡ (31.3%)\n",
      "  AI åˆ›æŠ•ç”Ÿæ€åœˆ: 1 æ¡ (1.5%)\n",
      "\n",
      "Topic 6 (37 æ¡):\n",
      "  AI æ—¶ä»£çš„æˆ‘ä»¬: 17 æ¡ (45.9%)\n",
      "  AI å·¥å…·æµ‹è¯„ä¸­å¿ƒ: 8 æ¡ (21.6%)\n",
      "  AIä¸äººç±»æœªæ¥: 4 æ¡ (10.8%)\n",
      "\n",
      "Topic 7 (399 æ¡):\n",
      "  AIå†™ä½œç ”ç©¶æ‰€: 106 æ¡ (26.6%)\n",
      "  AIä¸äººç±»æœªæ¥: 84 æ¡ (21.1%)\n",
      "  AI å®‰å…¨ç ”ç©¶æ‰€: 37 æ¡ (9.3%)\n",
      "\n",
      "Topic 8 (32 æ¡):\n",
      "  AI å·¥å…·æµ‹è¯„ä¸­å¿ƒ: 11 æ¡ (34.4%)\n",
      "  AI æ—¶ä»£çš„æˆ‘ä»¬: 9 æ¡ (28.1%)\n",
      "  AIä¸äººç±»æœªæ¥: 5 æ¡ (15.6%)\n"
     ]
    }
   ],
   "source": [
    "## ä¸»é¢˜ä¸åœˆå­çš„äº¤å‰åˆ†æ\n",
    "\n",
    "df_result = df.copy()\n",
    "df_result['topic_id'] = labels\n",
    "\n",
    "# ä¸»é¢˜ä¸åœˆå­çš„äº¤å‰è¡¨\n",
    "cross_tab = pd.crosstab(df_result['topic_id'], df_result['ring_name'])\n",
    "print(\"\\nä¸»é¢˜åœ¨å„åœˆå­çš„åˆ†å¸ƒ:\")\n",
    "print(cross_tab)\n",
    "\n",
    "# æ¯ä¸ªä¸»é¢˜æœ€ä¸»è¦çš„åœˆå­\n",
    "print(\"\\næ¯ä¸ªä¸»é¢˜çš„ä¸»è¦åœˆå­:\")\n",
    "for topic in sorted(df_result['topic_id'].unique()):\n",
    "    if topic == -1:\n",
    "        continue\n",
    "    topic_data = df_result[df_result['topic_id'] == topic]\n",
    "    top_rings = topic_data['ring_name'].value_counts().head(3)\n",
    "    print(f\"\\nTopic {topic} ({len(topic_data)} æ¡):\")\n",
    "    for ring, count in top_rings.items():\n",
    "        print(f\"  {ring}: {count} æ¡ ({count/len(topic_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-poa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
